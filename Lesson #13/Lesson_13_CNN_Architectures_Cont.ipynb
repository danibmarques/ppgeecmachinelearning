{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson #13 - CNN Architectures Cont.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9M6TvG9l022"
      },
      "source": [
        "# 1.0 Network as feature extractors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhyRH1BkmRqi"
      },
      "source": [
        "Over this lesson, we’ll be discussing the concept of **transfer learning**, \n",
        "\n",
        "> the ability to use a pre-trained model as a “shortcut” to learn patterns from data it was not originally trained on.\n",
        "\n",
        "Consider a traditional machine learning scenario where we are given two classification challenges.\n",
        "\n",
        "**In the first challenge**, our goal is to train a Convolutional Neural Network to recognize dogs\n",
        "vs. cats in an image.\n",
        "\n",
        "Then, **in the second project**, we are tasked with recognizing three separate species of bears:\n",
        "grizzly bears, polar bears, and giant pandas. Using standard practices in **machine learning, neural networks, and deep learning**, we would treat these challenges as two separate problems. \n",
        "\n",
        "- First, we would gather a sufficient labeled dataset of dogs and cats, followed by training a model on the dataset. \n",
        "- We would then repeat the process a second time, only this time, gathering images of our\n",
        "bear breeds, and then training a model on top of the labeled bear dataset.\n",
        "\n",
        "\n",
        "Transfer learning proposes a different training paradigm – \n",
        "\n",
        "> what if we could use an existing pretrained classifier and use it as a starting point for a new classification task?\n",
        "\n",
        "In context of the proposed challenges above, **we would first train a Convolutional Neural Network to recognize dogs versus cats**. \n",
        "\n",
        "> Then, we would use the same CNN trained on dog and cat data to be used to\n",
        "distinguish between bear classes, even though no bear data was mixed with the dog and cat data.\n",
        "\n",
        "\n",
        "Does this sound too good to be true? It’s actually not. **Deep neural networks trained on\n",
        "large-scale datasets such as ImageNet have demonstrated to be excellent at the task of transfer\n",
        "learning**. These networks learn a set of rich, discriminating features to recognize 1,000 separate object classes. It makes sense that these filters can be reused for classification tasks other than what the CNN was originally trained on.\n",
        "\n",
        "In general, **there are two types of transfer learning** when applied to deep learning for **computer vision**:\n",
        "\n",
        "1. Treating networks as arbitrary feature extractors.\n",
        "2. Removing the fully-connected layers of an existing network, placing new FC layer set on\n",
        "top of the CNN, and fine-tuning these weights (and optionally previous layers) to recognize\n",
        "object classes.\n",
        "\n",
        "\n",
        "In this section, we’ll be focusing primarily on the first method of transfer learning, treating networks as feature extractors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLv0dSxrnwwK"
      },
      "source": [
        "## 1.1 Extracting features with a pre-trained CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAGBKpo2n-Zq"
      },
      "source": [
        "Up until this point, we have treated Convolutional Neural Networks as end-to-end image classifiers:\n",
        "\n",
        "1. We input an image to the network.\n",
        "2. The image forward propagates through the network.\n",
        "3. We obtain the final classification probabilities from the end of the network.\n",
        "\n",
        "However, **there is no “rule” that says we must allow the image to forward propagate through\n",
        "the entire network**. \n",
        "\n",
        "> Instead, we can stop the propagation at an arbitrary layer, such as an activation\n",
        "or pooling layer, extract the values from the network at this time, and then use them as feature\n",
        "vectors. \n",
        "\n",
        "For example, let’s consider the VGG16 network architecture by [Simonyan and Zisserman](https://arxiv.org/abs/1409.1556) (Figure below, left).\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1CNy_EpEVeVAn7LJbPyeZm7xKW1rLnwdz\"></center><center><b>Left</b>: The original VGG16 network architecture that outputs probabilities for each of the 1,000 ImageNet class labels. <b>Right</b>: Removing the FC layers from VGG16 and instead returning the output of the final POOL layer. This output will serve as our extracted features.</center>\n",
        "\n",
        "Along with the layers in the network, we have also included the input and output shapes of the\n",
        "volumes for each layer. When treating networks as a feature extractor, we essentially “chop off” the network at an arbitrary point (normally prior to the fully-connected layers, but it really depends on your particular dataset).\n",
        "\n",
        "Now the last layer in our network is a max pooling layer (Figure above, right) which will have the output shape of 7 x 7 x 512 implying there are 512 filters each of size 7 x 7. If we were to forward propagate an image through this network with its FC head removed, we would be left with 512, 7x7 activations that have either activated or not based on the image contents. Therefore, we can actually take these 7x7x512 = 25,088 values and treat them as a feature vector that **quantifies the contents of an image**.\n",
        "\n",
        "If we repeat this process for an entire dataset of images (including datasets that VGG16 was\n",
        "not trained on), we’ll be left with a design matrix of N images, each with 25,088 columns used to\n",
        "quantify their contents (i.e., feature vectors). Given our feature vectors, we can train an off-the-shelf machine learning model such a Linear SVM, Logistic Regression classifier, or Random Forest on top of these features to obtain a classifier that recognizes new classes of images.\n",
        "\n",
        "Keep in mind that the CNN itself is not capable of recognizing these new classes – instead,\n",
        "we are using the CNN as an intermediary feature extractor. The downstream machine learning\n",
        "classifier will take care of learning the underlying patterns of the features extracted from the CNN.\n",
        "\n",
        "Later in this section, we’ll be demonstrating how you can use pre-trained CNNs (specifically\n",
        "VGG16) and the Keras library to obtain > 90% classification accuracy on image datasets such as\n",
        "Animals, CALTECH-101, and Flowers-17. Neither of these datasets contain images that VGG16\n",
        "was trained on, but by applying transfer learning, we are able to build super accurate image\n",
        "classifiers with little effort. The trick is extracting these features and storing them in an efficient manner. To accomplish this task, we’ll need HDF5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAB44gZbfYAq"
      },
      "source": [
        "## 1.2 What is HDF5?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY9ZvSgVfdxF"
      },
      "source": [
        "**HDF5** is binary data format created by the [HDF5 group](https://www.hdfgroup.org/solutions/hdf5/) to store gigantic numerical datasets on disk (far too large to store in memory) while facilitating easy access and computation on the rows of the datasets. \n",
        "\n",
        "> Data in HDF5 is stored hierarchically, similar to how a file system stores data. \n",
        "\n",
        "Data is first defined in groups, where a group is a container-like structure which can hold datasets and other\n",
        "groups. Once a group has been defined, a dataset can be created within the group. A dataset can be\n",
        "thought of as a multi-dimensional array (i.e., a NumPy array) of a homogeneous data type (integer,\n",
        "float, unicode, etc.). An example of an HDF5 file containing a group with multiple datasets is\n",
        "displayed in Figure below.\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1-oiHlD5B97FTA3p9pnnm5PJhS4ywI_f_\"></center><center>An example of a HDF5 file with three datasets. The first dataset contains the\n",
        "label_names for CALTECH-101. We then have labels, which maps the each image to its\n",
        "corresponding class label. Finally, the features dataset contains the image quantifications extracted\n",
        "by the CNN</center>\n",
        "\n",
        "**HDF5 is written in C**; however, by using the [h5py module](h5py.org), we can gain access to\n",
        "the underlying C API using the Python programming language. What makes **h5py** so awesome\n",
        "is the **ease of interaction with data**. \n",
        "\n",
        "> We can store huge amounts of data in our HDF5 dataset and manipulate the data in a NumPy-like fashion. \n",
        "\n",
        "For example, we can use standard Python syntax to access and slice rows from multi-terabyte datasets stored on disk as if they were simple NumPy arrays loaded into memory. Thanks to specialized data structures, these slices and row accesses are lighting quick.\n",
        "\n",
        "When using HDF5 with h5py, **you can think of your data as a gigantic NumPy array**\n",
        "that is too large to fit into main memory but can still be accessed and manipulated just the same.\n",
        "Perhaps best of all, **the HDF5 format is standardized**\n",
        "\n",
        "> meaning that datasets stored in HDF5 format are inherently portable and can be accessed by other developers using different programming languages such as C, MATLAB, and Java.\n",
        "\n",
        "We’ll be writing a custom Python class that allows us to efficiently accept input data and write it to an HDF5 dataset. This class will then serve two purposes:\n",
        "\n",
        "1. Facilitate a method for us to apply transfer learning by taking our extracted features from\n",
        "a Deep Learning Architecture and writing them to an HDF5 dataset in an efficient manner.\n",
        "2. Allow us to generate HDF5 datasets from raw images to facilitate faster training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zisp6OplmCSt"
      },
      "source": [
        "## 1.3 Writing features to an HDF5 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96t7jk8mOXZ"
      },
      "source": [
        "Before we can even think about treating CNN Architectures as a feature extractor, we\n",
        "first need to develop a bit of infrastructure. In particular, we need to define a Python class\n",
        "named HDF5DatasetWriter, which as the name suggests, is responsible for taking an input set of\n",
        "NumPy arrays (whether features, raw images, etc.) and writing them to HDF5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT62Vkhv2NZ"
      },
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n",
        "    \"\"\"\n",
        "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
        "    \n",
        "    Args:\n",
        "    dims: controls the dimension or shape of the data we will be storing in the dataset.\n",
        "    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n",
        "    then dims=(70000, 784).\n",
        "    outputPath: path to where our output HDF5 file will be stored on disk.\n",
        "    datakey: The optional dataKey is the name of the dataset that will store\n",
        "    the data our algorithm will learn from.\n",
        "    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n",
        "    vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # check to see if the output path exists, and if so, raise\n",
        "    # an exception\n",
        "    if os.path.exists(outputPath):\n",
        "      raise ValueError(\"The supplied `outputPath` already \"\n",
        "        \"exists and cannot be overwritten. Manually delete \"\n",
        "        \"the file before continuing.\", outputPath)\n",
        "\n",
        "    # open the HDF5 database for writing and create two datasets:\n",
        "    # one to store the images/features and another to store the\n",
        "    # class labels\n",
        "    self.db = h5py.File(outputPath, \"w\")\n",
        "    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\")\n",
        "    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n",
        "\n",
        "    # store the buffer size, then initialize the buffer itself\n",
        "    # along with the index into the datasets\n",
        "    self.bufSize = bufSize\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add(self, rows, labels):\n",
        "    # add the rows and labels to the buffer\n",
        "    self.buffer[\"data\"].extend(rows)\n",
        "    self.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "    # check to see if the buffer needs to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "      self.flush()\n",
        "\n",
        "  def flush(self):\n",
        "    # write the buffers to disk then reset the buffer\n",
        "    i = self.idx + len(self.buffer[\"data\"])\n",
        "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
        "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "    self.idx = i\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "  def storeClassLabels(self, classLabels):\n",
        "    # create a dataset to store the actual class label names,\n",
        "    # then store the class labels\n",
        "    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n",
        "    labelSet[:] = classLabels\n",
        "\n",
        "  def close(self):\n",
        "    # check to see if there are any other entries in the buffer\n",
        "    # that need to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) > 0:\n",
        "      self.flush()\n",
        "\n",
        "    # close the dataset\n",
        "    self.db.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3uegkGTcPdB"
      },
      "source": [
        "As you can see, the **HDF5DatasetWriter** doesn’t have much to do with machine learning or\n",
        "deep learning at all – it’s simply a class used to help us store data in HDF5 format. As you continue\n",
        "in your deep learning career, you’ll notice that much of the initial labor when setting up a new\n",
        "problem is getting the data into a format you can work with. Once you have your data in a format\n",
        "that’s straightforward to manipulate, it becomes substantially easier to apply machine learning and\n",
        "deep learning techniques to your data.\n",
        "\n",
        "Now that our **HDF5DatasetWriter** is implemented, we can move on to actually extracting\n",
        "features using pre-trained Convolutional Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgghb6qvqaXD"
      },
      "source": [
        "## 1.4 The feature extraction process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MiEPB9jwSZG"
      },
      "source": [
        "Let’s define a Python script that can be used to extract features from an arbitrary image dataset (provided the input dataset follows a specific directory structure)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MagMf-0LkCin"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc1mxvEANFRU",
        "outputId": "03c85d89-e2a8-4539-b8df-3370b9541eac"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCUM0shSMYWF",
        "outputId": "0efbe9ba-ff99-4386-bc36-b2336805e7af"
      },
      "source": [
        "# download animals dataset\n",
        "!gdown https://drive.google.com/uc?id=1ZkrEbDEdiSjpog6IcWK-HB2Y3uk2WjFE\n",
        "\n",
        "# download caltech-101 dataset\n",
        "!gdown https://drive.google.com/uc?id=1VpcNjEFHbtfZbQx7Q9FvRBlCYFwxYPIS\n",
        "\n",
        "# download flowers dataset\n",
        "!gdown https://drive.google.com/uc?id=1o_BeSmvyuelAyEYpGPNphlkX4bfQy2r5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZkrEbDEdiSjpog6IcWK-HB2Y3uk2WjFE\n",
            "To: /content/animals.zip\n",
            "197MB [00:01, 141MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VpcNjEFHbtfZbQx7Q9FvRBlCYFwxYPIS\n",
            "To: /content/caltech-101.zip\n",
            "121MB [00:01, 70.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o_BeSmvyuelAyEYpGPNphlkX4bfQy2r5\n",
            "To: /content/flowers17.zip\n",
            "60.5MB [00:00, 146MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpYqrMqMNSSq"
      },
      "source": [
        "!unzip animals.zip\n",
        "!unzip caltech-101.zip\n",
        "!unzip flowers17.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ny6w-yq4Ls"
      },
      "source": [
        "def feature_extraction(dataset,output,buffer_size,bs):\n",
        "\t\t'''\n",
        "\t\t\tdataset: input folder with images dataset\n",
        "\t\t\toutput: folder to store the feature extraction\n",
        "\t\t\tbuffer_size: controls the size of our in-memory buffer\n",
        "\t\t\tbs: batch size\n",
        "\t\t'''\n",
        "\n",
        "\t\t# grab the list of images that we'll be describing then randomly\n",
        "\t\t# shuffle them to allow for easy training and testing splits via\n",
        "\t\t# array slicing during training time\n",
        "\t\tprint(\"[INFO] loading images...\")\n",
        "\t\timagePaths = list(paths.list_images(dataset))\n",
        "\t\trandom.shuffle(imagePaths)\n",
        "\n",
        "\t\t# extract the class labels from the image paths then encode the\n",
        "\t\t# labels\n",
        "\t\tlabels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
        "\t\tle = LabelEncoder()\n",
        "\t\tlabels = le.fit_transform(labels)\n",
        "\n",
        "\t\t# load the VGG16 network\n",
        "\t\tprint(\"[INFO] loading network...\")\n",
        "\t\tmodel = VGG16(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "\t\t# initialize the HDF5 dataset writer, then store the class label\n",
        "\t\t# names in the dataset\n",
        "\t\tdataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toutput, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdataKey=\"features\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbufSize=buffer_size)\n",
        "\t\tdataset.storeClassLabels(le.classes_)\n",
        "\n",
        "\t\t# initialize the progress bar\n",
        "\t\twidgets = [\"Extracting Features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "\t\tpbar = progressbar.ProgressBar(maxval=len(imagePaths),widgets=widgets).start()\n",
        "\n",
        "\t\t# loop over the images in batches\n",
        "\t\tfor i in np.arange(0, len(imagePaths), bs):\n",
        "\t\t\t# extract the batch of images and labels, then initialize the\n",
        "\t\t\t# list of actual images that will be passed through the network\n",
        "\t\t\t# for feature extraction\n",
        "\t\t\tbatchPaths = imagePaths[i:i + bs]\n",
        "\t\t\tbatchLabels = labels[i:i + bs]\n",
        "\t\t\tbatchImages = []\n",
        "\n",
        "\t\t\t# loop over the images and labels in the current batch\n",
        "\t\t\tfor (j, imagePath) in enumerate(batchPaths):\n",
        "\t\t\t\t# load the input image using the Keras helper utility\n",
        "\t\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
        "\t\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\t\t\t\timage = img_to_array(image)\n",
        "\n",
        "\t\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
        "\t\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
        "\t\t\t\t# ImageNet dataset\n",
        "\t\t\t\timage = np.expand_dims(image, axis=0)\n",
        "\t\t\t\timage = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "\t\t\t\t# add the image to the batch\n",
        "\t\t\t\tbatchImages.append(image)\n",
        "\n",
        "\t\t\t# pass the images through the network and use the outputs as\n",
        "\t\t\t# our actual features\n",
        "\t\t\tbatchImages = np.vstack(batchImages)\n",
        "\t\t\tfeatures = model.predict(batchImages, batch_size=bs)\n",
        "\n",
        "\t\t\t# reshape the features so that each image is represented by\n",
        "\t\t\t# a flattened feature vector of the `MaxPooling2D` outputs\n",
        "\t\t\tfeatures = features.reshape((features.shape[0], 512 * 7 * 7))\n",
        "\n",
        "\t\t\t# add the features and labels to our HDF5 dataset\n",
        "\t\t\tdataset.add(features, batchLabels)\n",
        "\t\t\tpbar.update(i)\n",
        "\n",
        "\t\t# close the dataset\n",
        "\t\tdataset.close()\n",
        "\t\tpbar.finish()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6N03sCR9uP"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "def train_and_evaluate(features_set):\n",
        "    db = h5py.File(features_set,mode='r')\n",
        "    print(\"Database keys {0:}\".format(list(db.keys())))\n",
        "\n",
        "    # open the HDF5 database for reading then determine the index of\n",
        "    # the training and testing split, provided that this data was\n",
        "    # already shuffled *prior* to writing it to disk\n",
        "    i = int(db[\"labels\"].shape[0] * 0.75)\n",
        "\n",
        "    # define the set of parameters that we want to tune then start a\n",
        "    # grid search where we evaluate our model for each value of C\n",
        "    print(\"[INFO] tuning hyperparameters...\")\n",
        "    params = {\"C\": [0.1, 1.0, 10.0]}\n",
        "    model = GridSearchCV(LogisticRegression(solver=\"lbfgs\",\n",
        "                                            multi_class=\"auto\"),\n",
        "                        params, \n",
        "                        cv=3, \n",
        "                        n_jobs=-1)\n",
        "\n",
        "    model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "    print(\"[INFO] best hyperparameters: {}\".format(model.best_params_))\n",
        "\n",
        "    # evaluate the model\n",
        "    print(\"[INFO] evaluating...\")\n",
        "    preds = model.predict(db[\"features\"][i:])\n",
        "    print(classification_report(db[\"labels\"][i:], \n",
        "                                preds,\n",
        "                                target_names=db[\"label_names\"]))\n",
        "\n",
        "    # serialize the model to disk\n",
        "    print(\"[INFO] saving model...\")\n",
        "    f = open(features_set.split(\"/\")[0] + \".cpickle\", \"wb\")\n",
        "    f.write(pickle.dumps(model.best_estimator_))\n",
        "    f.close()\n",
        "\n",
        "    # close the database\n",
        "    db.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kana_F16RG5x"
      },
      "source": [
        "### 1.4.1 Animals dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veqc-zoW3WZk"
      },
      "source": [
        "\n",
        "The first dataset we are going to extract features from using VGG16 is our “Animals” dataset. This dataset consists of 3,000 images, of three classes: dogs, cats, and pandas. Notice how the .shape is (3000, 25088) – this result implies that each of the 3,000 images in our Animals dataset is quantified via feature vector with length 25,088 (i.e., the values inside **VGG16** after the final POOL operation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIgpmBPaPnS7"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"animals\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"animals/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOzS-WGuQSB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c0ce2d-8b6c-4bd8-942b-488e2e070352"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:00:38\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lurxOifvu1ul",
        "outputId": "7ff4f1ea-3331-4142-c187-ef1c902a1dd7"
      },
      "source": [
        "db = h5py.File(output,mode='r')\n",
        "list(db.keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label_names', 'labels']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TMfKV702WwL",
        "outputId": "b6f5042a-f1fb-4673-b54a-81635533d040"
      },
      "source": [
        "db[\"features\"].shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 25088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD0Jv4qm3dOr",
        "outputId": "c5f3772d-599c-405d-a81d-c3c758c8d901"
      },
      "source": [
        "db[\"labels\"].shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYNURAtd3gxH",
        "outputId": "8f419d71-359a-40a6-a142-759e249bc858"
      },
      "source": [
        "db[\"label_names\"].shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FkQSXiRTJYs",
        "outputId": "79ad905a-4707-4ac2-ef44-0c659f35e69a"
      },
      "source": [
        "train_and_evaluate(output)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n",
            "[INFO] best hyperparameters: {'C': 10.0}\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.97      1.00      0.98       233\n",
            "        dogs       1.00      0.97      0.98       261\n",
            "       panda       0.99      1.00      1.00       256\n",
            "\n",
            "    accuracy                           0.99       750\n",
            "   macro avg       0.99      0.99      0.99       750\n",
            "weighted avg       0.99      0.99      0.99       750\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_1OqSFhUtMe"
      },
      "source": [
        "### 1.4.2 Caltech-101 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RHqW_CaUtMe"
      },
      "source": [
        "Just as we extracted features from the Animals dataset, we can do the same with CALTECH-101."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyQKIQmtUtMe"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"caltech-101\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"caltech-101/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKr-Wwj0UtMf",
        "outputId": "b65e6dc2-2fa0-445e-a011-5c0ac782ee72"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:01:06\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeEuL-TuUtMf",
        "outputId": "d6628cc6-268e-42b4-ec8f-5cae4dea10f2"
      },
      "source": [
        "db = h5py.File(output,mode='r')\n",
        "list(db.keys())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label_names', 'labels']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqKuEarwUtMf",
        "outputId": "e948cdd3-757e-4a53-c41d-6c044756a408"
      },
      "source": [
        "db[\"features\"].shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8677, 25088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4eakXQtUtMg",
        "outputId": "6b2d8b07-fa76-41cc-a3ed-c309e4e447d8"
      },
      "source": [
        "db[\"labels\"].shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8677,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giLEoXUCUtMg",
        "outputId": "bb8f9621-cdfa-4d3d-fa2e-9db9039aae04"
      },
      "source": [
        "db[\"label_names\"].shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZtFX3x5UtMg"
      },
      "source": [
        "train_and_evaluate(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrSOmdsYhJJf"
      },
      "source": [
        "### 1.4.3 Flowers-17 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSJT0zNhJJf"
      },
      "source": [
        "Just as we extracted features from the Animals dataset, we can do the same with CALTECH-101."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csu5b3HXhJJf"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"flowers17\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"flowers17/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsT5dL5nhJJg",
        "outputId": "5cb20f86-f9da-4853-eb9b-7c2e0c1fabd3"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:00:17\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fid7qxUhJJh",
        "outputId": "bd8b0f15-c5db-479d-e23a-235e3c9ae378"
      },
      "source": [
        "db = h5py.File(output,mode='r')\n",
        "list(db.keys())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label_names', 'labels']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVYAf-cUhJJi",
        "outputId": "71081233-b3f5-4484-9dba-30b151004bdd"
      },
      "source": [
        "db[\"features\"].shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1360, 25088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwpPo9WUhJJi",
        "outputId": "91f92c4f-4907-42e1-cdff-6f33bb6443de"
      },
      "source": [
        "db[\"labels\"].shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1360,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfeDcLIzhJJi",
        "outputId": "59c30a3c-7cfb-405f-8baf-baad90aa2475"
      },
      "source": [
        "db[\"label_names\"].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dT7THsbhJJi",
        "outputId": "e39598f7-f991-4c86-c209-4bd13b908ce0"
      },
      "source": [
        "train_and_evaluate(output)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n",
            "[INFO] best hyperparameters: {'C': 10.0}\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.86      0.90      0.88        21\n",
            "   buttercup       0.95      0.90      0.92        20\n",
            "   coltsfoot       0.89      0.70      0.78        23\n",
            "     cowslip       0.69      0.87      0.77        23\n",
            "      crocus       0.94      0.94      0.94        18\n",
            "    daffodil       0.88      0.84      0.86        25\n",
            "       daisy       1.00      1.00      1.00        21\n",
            "   dandelion       0.76      0.84      0.80        19\n",
            "  fritillary       1.00      0.88      0.94        17\n",
            "        iris       0.95      0.83      0.88        23\n",
            "  lilyvalley       0.87      0.87      0.87        23\n",
            "       pansy       1.00      0.96      0.98        25\n",
            "    snowdrop       0.73      0.95      0.83        20\n",
            "   sunflower       1.00      1.00      1.00        15\n",
            "   tigerlily       0.94      0.94      0.94        17\n",
            "       tulip       0.79      0.69      0.73        16\n",
            "  windflower       0.86      0.86      0.86        14\n",
            "\n",
            "    accuracy                           0.88       340\n",
            "   macro avg       0.89      0.88      0.88       340\n",
            "weighted avg       0.89      0.88      0.88       340\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRy_5eGBhJJi"
      },
      "source": [
        "# 2.0 Fine-tuning networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVh5JwSiZrj"
      },
      "source": [
        "In the previous section we learned how to treat a pre-trained **Convolutional Neural Network** as **feature extractor**. \n",
        "\n",
        "> Using this feature extractor, we forward propagated our dataset of images through the network, extracted the activations at a given layer, and saved the values to disk. A standard machine\n",
        "learning classifier (in this case, Logistic Regression) was then trained on top of the CNN features.\n",
        "\n",
        "This CNN feature extractor approach, called **transfer learning**, obtained remarkable accuracy, far higher than any of our previous experiments on the Animals, CALTECH-101, or Flowers-17 dataset.\n",
        "\n",
        "But there is another type of transfer learning, one that can actually outperform the feature extraction method if you have sufficient data. This method is called **fine-tuning** and **requires us to perform “network surgery”**. \n",
        "\n",
        "1. First, we take a **scalpel and cut off the final set of fully-connected layers** (i.e., the “head” of the network) from a pre-trained /Convolutional Neural Network, such as\n",
        "VGG, ResNet, or Inception. \n",
        "2. We then **replace the head** with a new set of fully-connected layers with random initializations. From there all layers below the head are frozen so their weights cannot be\n",
        "updated (i.e., the backward pass in backpropagation does not reach them)\n",
        "3.  We then train the network **using a very small learning rate** so the new set of FC layers can start to learn patterns from the previously learned CONV layers earlier in the network. \n",
        "4. Optionally, we may unfreeze the rest of the network and continue training. Applying fine-tuning allows us to apply pre-trained networks to recognize classes that they were not originally trained on; furthermore, **this method can lead to higher accuracy than feature extraction**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpP_T-fknnS"
      },
      "source": [
        "## 2.1 Transfer Learning and Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGXnCSBksDi"
      },
      "source": [
        "**Fine-tuning is a type of transfer learning**. We apply fine-tuning to deep learning models that have already been trained on a given dataset. Typically, these networks are state-of-the-art architectures\n",
        "such as VGG, ResNet, and Inception that have been trained on the ImageNet dataset.\n",
        "\n",
        "As we found out in previous section on feature extraction, these networks contain rich, discriminative filters that can be used on datasets and class labels outside the ones they have already been trained on. However, instead of simply applying feature extraction, we are going to perform network surgery and modify the actual architecture so we can re-train parts of the network.\n",
        "\n",
        "\n",
        "If this sounds like something out of a bad horror movie; don’t worry, there won’t be any blood and gore – but we will have some fun and learn a lot with our experiments. To understand how finetuning\n",
        "works, consider Figure below (left) where we have the layers of the VGG16 network. As we know, the final set of layers (i.e., the “head”) are our fully-connected layers along with our softmax classifier. When performing fine-tuning, we actually remove the head from the network, just as in feature extraction (middle). However, unlike feature extraction, when we perform fine-tuning we actually **build a new fully-connected head and place it on top of the original architecture\n",
        "(right)**.\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1qTj4KeosAyDUcffqTQ_BepiINEUXs-cE\"></center><center><b>Left</b>:  The original VGG16 network architecture. <b>Middle</b>: Removing the FC layers from VGG16 and treating the final POOL layer as a feature extractor. <b>Right</b>: Removing the original FC layers and replacing them with a brand new FC head. These new FC layers can then be fine-tuned to the specific dataset (the old FC layers are no longer used).</center>\n",
        "\n",
        "\n",
        "In most cases your new FC head will have fewer parameters than the original one; however, that really depends on your particular dataset. The new FC head is randomly initialized (just like any other layer in a new network) and connected to the body of the original network, and we are ready to train.\n",
        "\n",
        "However, there is a problem – our CONV layers have already learned rich, discriminating filters while our FC layers are brand new and totally random. If we allow the gradient to backpropagate from these random values all the way through the body of our network, we risk destroying these powerful features. To circumvent this, we instead let our FC head “warm up” by (ironically) “freezing” all layers in the body of the network (I told you the cadaver analogy works well here) as\n",
        "in Figure below (left).\n",
        "\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=11Zh6mGG3qMISsnCg6JLgL-sH7TnxpUSC\"></center><center><b>Left</b>: When we start the fine-tuning process we freeze all CONV layers in the network and only allow the gradient to backpropagate through the FC layers. Doing this allows our network to “warm up”. <b>Right</b>: After the FC layers have had a chance to warm up we may choose to unfreeze all layers in the network and allow each of them to be fine-tuned as well.</center>\n",
        "\n",
        "\n",
        "Training data is forward propagated through the network as we normally would; however, the backpropagation is stopped after the FC layers, which allows these layers to start to learn patterns from the highly discriminative CONV layers. In some cases, we may never unfreeze the body of the network as our new FC head may obtain sufficient accuracy. \n",
        "\n",
        "However, for some datasets it is often advantageous to allow the original CONV layers to be modified during the fine-tuning process as\n",
        "well (Figure above, right).\n",
        "\n",
        "After the FC head has started to learn patterns in our dataset, pause training, unfreeze the body, and then continue the training, but with a very **small learning rate** – we do not want to deviate our\n",
        "CONV filters dramatically. \n",
        "\n",
        "Training is then allowed to continue until sufficient accuracy is obtained. Fine-tuning is a super powerful method to obtain image classifiers from pre-trained CNNs on custom datasets, even more powerful than feature extraction in most cases. **The downside is that\n",
        "fine-tuning can require a bit more work and your choice in FC head parameters does play a big part\n",
        "in network accuracy** – you can’t rely strictly on regularization techniques here as your network has already been pre-trained and you can’t deviate from the regularization already being performed by\n",
        "the network.\n",
        "\n",
        "Secondly, for small datasets, it can be challenging to get your network to start “learning” from a “cold” FC start, which is why we freeze the body of the network first. Even still, getting past the warm-up stage can be a bit of a challenge and might require you to use optimizers other than SGD. **While fine-tuning does require a bit more effort, if it is done correctly, you’ll nearly always enjoy higher accuracy**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQ6aKJgaalh"
      },
      "source": [
        "## 2.2 Indexes and Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p37ZtS0IwXBs"
      },
      "source": [
        "Prior to performing **network surgery**, we need to know the **layer name and index** of every layer in a given deep learning model. We need this information as we’ll be required to **“freeze”** and **“unfreeze”** certain layers in a pre-trained CNN.\n",
        "\n",
        "Without knowing the layer names and indexes ahead of time, we would be “cutting blindly”, an out-of-control surgeon with no game plan. **If we instead take a few minutes to examine the network architecture and implementation, we can better prepare for our surgery.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ-1lFYebUiP",
        "outputId": "7a52140f-e9f8-43c7-a0b3-9b953e69e0b8"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# whether or not to include top of CNN\n",
        "include_top = 0\n",
        "\n",
        "# load the VGG16 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top= include_top > 0)\n",
        "print(\"[INFO] showing layers...\")\n",
        "\n",
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "[INFO] showing layers...\n",
            "[INFO] 0\tInputLayer\n",
            "[INFO] 1\tConv2D\n",
            "[INFO] 2\tConv2D\n",
            "[INFO] 3\tMaxPooling2D\n",
            "[INFO] 4\tConv2D\n",
            "[INFO] 5\tConv2D\n",
            "[INFO] 6\tMaxPooling2D\n",
            "[INFO] 7\tConv2D\n",
            "[INFO] 8\tConv2D\n",
            "[INFO] 9\tConv2D\n",
            "[INFO] 10\tMaxPooling2D\n",
            "[INFO] 11\tConv2D\n",
            "[INFO] 12\tConv2D\n",
            "[INFO] 13\tConv2D\n",
            "[INFO] 14\tMaxPooling2D\n",
            "[INFO] 15\tConv2D\n",
            "[INFO] 16\tConv2D\n",
            "[INFO] 17\tConv2D\n",
            "[INFO] 18\tMaxPooling2D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64KpW4KzcBL"
      },
      "source": [
        "Before we can replace the head of a pre-trained CNN, we need something to replace it with – therefore, we need to define our own fully-connected head of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZw23cpCeZ_m"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# a fully connect network\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, D):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bZQdHFwzq7t"
      },
      "source": [
        "Again, this fully-connected head is very simplistic compared to the original head from VGG16 which consists of two sets of 4,096 FC layers. However, for most fine-tuning problems you are not seeking to replicate the original head of the network, but rather simplify it so it is easier to fine-tune– the fewer parameters in the head, the more likely we’ll be to correctly tune the network to a new\n",
        "classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgmBX1ST1F8U"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-FgPH1NuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import imutils\n",
        "import cv2\n",
        "\n",
        "# useful class to help the resize of images\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYq8Q1S71sxt"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# helper to load images\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VTRu6F82ZY1"
      },
      "source": [
        "In some cases you’ll want to allow the entire body to be trainable; however, for deeper architectures with many parameters such as VGG, I suggest only unfreezing the top CONV layers and then continuing training. If classification accuracy continues to improve (without overfitting), you may want to consider unfreezing more layers in the body."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pZDxBAl0j3j",
        "outputId": "a2145cd0-f255-4e6a-f66a-4d0528bfebfb"
      },
      "source": [
        "\n",
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# \"path to input dataset\"\n",
        "dataset = \"flowers17\"\n",
        "\n",
        "# output model\n",
        "model_out = \"flowers17.model\"\n",
        "\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# grab the list of images that we'll be describing, then extract\n",
        "# the class label names from the image paths\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(dataset))\n",
        "classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
        "classNames = [str(x) for x in np.unique(classNames)]\n",
        "\n",
        "# initialize the image preprocessors\n",
        "aap = AspectAwarePreprocessor(224, 224)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# load the dataset from disk then scale the raw pixel intensities to\n",
        "# the range [0, 1]\n",
        "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
        "(data, labels) = sdl.load(imagePaths, verbose=500)\n",
        "data = data.astype(\"float\") / 255.0\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.25, random_state=42)\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "trainY = LabelBinarizer().fit_transform(trainY)\n",
        "testY = LabelBinarizer().fit_transform(testY)\n",
        "\n",
        "# load the VGG16 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n",
        "\n",
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "# RMSprop is frequently used in situations where we need to quickly obtain\n",
        "# reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).\n",
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network for a few epochs (all other\n",
        "# layers are frozen) -- this will allow the new FC layers to\n",
        "# start to become initialized with actual \"learned\" values\n",
        "# versus pure random\n",
        "print(\"[INFO] training head...\")\n",
        "model.fit(aug.flow(trainX, trainY, batch_size=32),\n",
        "                    validation_data=(testX, testY), epochs=25,\n",
        "                    steps_per_epoch=len(trainX) // 32, verbose=1)\n",
        "\n",
        "# evaluate the network after initialization\n",
        "print(\"[INFO] evaluating after initialization...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1), target_names=classNames))\n",
        "\n",
        "# now that the head FC layers have been trained/initialized, lets\n",
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "for layer in baseModel.layers[15:]:\n",
        "\tlayer.trainable = True\n",
        "\n",
        "# for the changes to the model to take affect we need to recompile\n",
        "# the model, this time using SGD with a *very* small learning rate\n",
        "print(\"[INFO] re-compiling model...\")\n",
        "opt = SGD(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "print(\"[INFO] fine-tuning model...\")\n",
        "model.fit(aug.flow(trainX, trainY, batch_size=32),\n",
        "                    validation_data=(testX, testY), epochs=100,\n",
        "                    steps_per_epoch=len(trainX) // 32, verbose=1)\n",
        "\n",
        "# evaluate the network on the fine-tuned model\n",
        "print(\"[INFO] evaluating after fine-tuning...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1), target_names=classNames))\n",
        "\n",
        "# save the model to disk\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(model_out)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] processed 500/1360\n",
            "[INFO] processed 1000/1360\n",
            "[INFO] compiling model...\n",
            "[INFO] training head...\n",
            "Epoch 1/25\n",
            "31/31 [==============================] - 14s 453ms/step - loss: 4.4924 - accuracy: 0.1508 - val_loss: 1.9294 - val_accuracy: 0.4088\n",
            "Epoch 2/25\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 2.2883 - accuracy: 0.2733 - val_loss: 1.6066 - val_accuracy: 0.5265\n",
            "Epoch 3/25\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 1.9376 - accuracy: 0.3836 - val_loss: 1.3217 - val_accuracy: 0.5941\n",
            "Epoch 4/25\n",
            "31/31 [==============================] - 13s 404ms/step - loss: 1.6892 - accuracy: 0.4595 - val_loss: 1.0731 - val_accuracy: 0.6882\n",
            "Epoch 5/25\n",
            "31/31 [==============================] - 13s 404ms/step - loss: 1.4986 - accuracy: 0.5121 - val_loss: 1.2966 - val_accuracy: 0.5676\n",
            "Epoch 6/25\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 1.4653 - accuracy: 0.5344 - val_loss: 0.8235 - val_accuracy: 0.7706\n",
            "Epoch 7/25\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 1.2692 - accuracy: 0.5830 - val_loss: 0.7980 - val_accuracy: 0.7706\n",
            "Epoch 8/25\n",
            "31/31 [==============================] - 13s 406ms/step - loss: 1.2099 - accuracy: 0.6093 - val_loss: 0.7693 - val_accuracy: 0.7735\n",
            "Epoch 9/25\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 1.1237 - accuracy: 0.6397 - val_loss: 0.6922 - val_accuracy: 0.7971\n",
            "Epoch 10/25\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 1.0880 - accuracy: 0.6589 - val_loss: 0.6827 - val_accuracy: 0.7853\n",
            "Epoch 11/25\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 1.1037 - accuracy: 0.6417 - val_loss: 0.7150 - val_accuracy: 0.7676\n",
            "Epoch 12/25\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.9486 - accuracy: 0.7075 - val_loss: 0.7057 - val_accuracy: 0.7882\n",
            "Epoch 13/25\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.9505 - accuracy: 0.6913 - val_loss: 0.6940 - val_accuracy: 0.8118\n",
            "Epoch 14/25\n",
            "31/31 [==============================] - 12s 403ms/step - loss: 0.8769 - accuracy: 0.7348 - val_loss: 0.6310 - val_accuracy: 0.8000\n",
            "Epoch 15/25\n",
            "31/31 [==============================] - 12s 400ms/step - loss: 0.8906 - accuracy: 0.7126 - val_loss: 0.7199 - val_accuracy: 0.7765\n",
            "Epoch 16/25\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.8778 - accuracy: 0.7136 - val_loss: 0.6193 - val_accuracy: 0.8441\n",
            "Epoch 17/25\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.7956 - accuracy: 0.7358 - val_loss: 0.5714 - val_accuracy: 0.8294\n",
            "Epoch 18/25\n",
            "31/31 [==============================] - 12s 403ms/step - loss: 0.7869 - accuracy: 0.7328 - val_loss: 0.5173 - val_accuracy: 0.8500\n",
            "Epoch 19/25\n",
            "31/31 [==============================] - 13s 404ms/step - loss: 0.7456 - accuracy: 0.7621 - val_loss: 0.5269 - val_accuracy: 0.8235\n",
            "Epoch 20/25\n",
            "31/31 [==============================] - 12s 402ms/step - loss: 0.7782 - accuracy: 0.7672 - val_loss: 0.5316 - val_accuracy: 0.8500\n",
            "Epoch 21/25\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.7697 - accuracy: 0.7449 - val_loss: 0.5242 - val_accuracy: 0.8471\n",
            "Epoch 22/25\n",
            "31/31 [==============================] - 13s 405ms/step - loss: 0.7618 - accuracy: 0.7672 - val_loss: 0.5299 - val_accuracy: 0.8412\n",
            "Epoch 23/25\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.6967 - accuracy: 0.7834 - val_loss: 0.5328 - val_accuracy: 0.8176\n",
            "Epoch 24/25\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.6006 - accuracy: 0.7966 - val_loss: 0.6280 - val_accuracy: 0.8000\n",
            "Epoch 25/25\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.6410 - accuracy: 0.7996 - val_loss: 0.5440 - val_accuracy: 0.8529\n",
            "[INFO] evaluating after initialization...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.72      0.90      0.80        20\n",
            "   buttercup       1.00      0.88      0.93        16\n",
            "   coltsfoot       1.00      0.89      0.94        18\n",
            "     cowslip       0.50      0.95      0.66        21\n",
            "      crocus       0.86      0.80      0.83        15\n",
            "    daffodil       0.92      0.85      0.88        27\n",
            "       daisy       0.96      0.96      0.96        23\n",
            "   dandelion       1.00      0.91      0.95        23\n",
            "  fritillary       1.00      0.65      0.79        23\n",
            "        iris       1.00      0.95      0.97        20\n",
            "  lilyvalley       0.88      0.70      0.78        20\n",
            "       pansy       0.95      0.95      0.95        19\n",
            "    snowdrop       0.62      1.00      0.77        18\n",
            "   sunflower       1.00      0.89      0.94        19\n",
            "   tigerlily       1.00      0.94      0.97        16\n",
            "       tulip       0.85      0.50      0.63        22\n",
            "  windflower       0.89      0.85      0.87        20\n",
            "\n",
            "    accuracy                           0.85       340\n",
            "   macro avg       0.89      0.86      0.86       340\n",
            "weighted avg       0.89      0.85      0.86       340\n",
            "\n",
            "[INFO] re-compiling model...\n",
            "[INFO] fine-tuning model...\n",
            "Epoch 1/100\n",
            "31/31 [==============================] - 13s 430ms/step - loss: 0.5348 - accuracy: 0.8219 - val_loss: 0.4457 - val_accuracy: 0.8765\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.4649 - accuracy: 0.8411 - val_loss: 0.4184 - val_accuracy: 0.8824\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.3971 - accuracy: 0.8649 - val_loss: 0.4158 - val_accuracy: 0.8647\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.4121 - accuracy: 0.8715 - val_loss: 0.4329 - val_accuracy: 0.8794\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.3758 - accuracy: 0.8816 - val_loss: 0.3771 - val_accuracy: 0.8912\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.3680 - accuracy: 0.8806 - val_loss: 0.3938 - val_accuracy: 0.8853\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.3611 - accuracy: 0.8806 - val_loss: 0.3703 - val_accuracy: 0.8853\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.3363 - accuracy: 0.8947 - val_loss: 0.3568 - val_accuracy: 0.8912\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.3211 - accuracy: 0.8957 - val_loss: 0.3718 - val_accuracy: 0.8941\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.3039 - accuracy: 0.8907 - val_loss: 0.4011 - val_accuracy: 0.8647\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.2975 - accuracy: 0.8937 - val_loss: 0.3180 - val_accuracy: 0.8971\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.3021 - accuracy: 0.8968 - val_loss: 0.3338 - val_accuracy: 0.8941\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.3024 - accuracy: 0.9008 - val_loss: 0.3394 - val_accuracy: 0.8882\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.2941 - accuracy: 0.9049 - val_loss: 0.3389 - val_accuracy: 0.8941\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 13s 416ms/step - loss: 0.2872 - accuracy: 0.9109 - val_loss: 0.3741 - val_accuracy: 0.8853\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.2834 - accuracy: 0.9049 - val_loss: 0.3736 - val_accuracy: 0.8912\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.3056 - accuracy: 0.8877 - val_loss: 0.3577 - val_accuracy: 0.8882\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.2748 - accuracy: 0.9028 - val_loss: 0.3522 - val_accuracy: 0.8824\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.2408 - accuracy: 0.9180 - val_loss: 0.3169 - val_accuracy: 0.9059\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.2780 - accuracy: 0.9038 - val_loss: 0.3349 - val_accuracy: 0.8912\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.2823 - accuracy: 0.8897 - val_loss: 0.3125 - val_accuracy: 0.9118\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.2660 - accuracy: 0.9079 - val_loss: 0.3199 - val_accuracy: 0.9029\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2567 - accuracy: 0.9089 - val_loss: 0.3630 - val_accuracy: 0.8824\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.2448 - accuracy: 0.9251 - val_loss: 0.3229 - val_accuracy: 0.9118\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.2659 - accuracy: 0.9038 - val_loss: 0.3329 - val_accuracy: 0.9029\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.2654 - accuracy: 0.9059 - val_loss: 0.3393 - val_accuracy: 0.8912\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 13s 405ms/step - loss: 0.2395 - accuracy: 0.9140 - val_loss: 0.3768 - val_accuracy: 0.8912\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 13s 404ms/step - loss: 0.2210 - accuracy: 0.9241 - val_loss: 0.3449 - val_accuracy: 0.8941\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2487 - accuracy: 0.9133 - val_loss: 0.3480 - val_accuracy: 0.8912\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2392 - accuracy: 0.9221 - val_loss: 0.2790 - val_accuracy: 0.9294\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.2240 - accuracy: 0.9231 - val_loss: 0.3283 - val_accuracy: 0.9029\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.2130 - accuracy: 0.9312 - val_loss: 0.3360 - val_accuracy: 0.8882\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.2613 - accuracy: 0.9130 - val_loss: 0.3159 - val_accuracy: 0.9118\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1710 - accuracy: 0.9403 - val_loss: 0.3246 - val_accuracy: 0.9029\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.2037 - accuracy: 0.9281 - val_loss: 0.2923 - val_accuracy: 0.9176\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.2387 - accuracy: 0.9200 - val_loss: 0.3105 - val_accuracy: 0.9059\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.2081 - accuracy: 0.9170 - val_loss: 0.2912 - val_accuracy: 0.9206\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 13s 416ms/step - loss: 0.2199 - accuracy: 0.9221 - val_loss: 0.2980 - val_accuracy: 0.9059\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.2250 - accuracy: 0.9190 - val_loss: 0.3392 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.2035 - accuracy: 0.9332 - val_loss: 0.2842 - val_accuracy: 0.9147\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.2013 - accuracy: 0.9281 - val_loss: 0.2992 - val_accuracy: 0.9088\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.2072 - accuracy: 0.9302 - val_loss: 0.3091 - val_accuracy: 0.8971\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.2049 - accuracy: 0.9271 - val_loss: 0.3343 - val_accuracy: 0.9059\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.1974 - accuracy: 0.9312 - val_loss: 0.3063 - val_accuracy: 0.9088\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.2241 - accuracy: 0.9170 - val_loss: 0.2841 - val_accuracy: 0.9176\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.1881 - accuracy: 0.9413 - val_loss: 0.2887 - val_accuracy: 0.9147\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.2024 - accuracy: 0.9281 - val_loss: 0.3029 - val_accuracy: 0.9176\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.2008 - accuracy: 0.9383 - val_loss: 0.2736 - val_accuracy: 0.9265\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.2029 - accuracy: 0.9302 - val_loss: 0.2706 - val_accuracy: 0.9206\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 13s 424ms/step - loss: 0.2086 - accuracy: 0.9180 - val_loss: 0.2892 - val_accuracy: 0.9294\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.2138 - accuracy: 0.9281 - val_loss: 0.2786 - val_accuracy: 0.9176\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1809 - accuracy: 0.9372 - val_loss: 0.2792 - val_accuracy: 0.9324\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.1690 - accuracy: 0.9423 - val_loss: 0.2964 - val_accuracy: 0.9206\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.1726 - accuracy: 0.9474 - val_loss: 0.2975 - val_accuracy: 0.9147\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1822 - accuracy: 0.9383 - val_loss: 0.3144 - val_accuracy: 0.9176\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.1962 - accuracy: 0.9403 - val_loss: 0.3058 - val_accuracy: 0.9206\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 13s 416ms/step - loss: 0.1659 - accuracy: 0.9453 - val_loss: 0.2850 - val_accuracy: 0.9206\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 13s 416ms/step - loss: 0.1775 - accuracy: 0.9443 - val_loss: 0.3002 - val_accuracy: 0.9147\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1723 - accuracy: 0.9423 - val_loss: 0.2923 - val_accuracy: 0.9176\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1552 - accuracy: 0.9474 - val_loss: 0.2907 - val_accuracy: 0.9294\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1485 - accuracy: 0.9585 - val_loss: 0.2937 - val_accuracy: 0.9235\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.1736 - accuracy: 0.9383 - val_loss: 0.2909 - val_accuracy: 0.9235\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.2144 - accuracy: 0.9170 - val_loss: 0.3098 - val_accuracy: 0.9059\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1875 - accuracy: 0.9372 - val_loss: 0.2661 - val_accuracy: 0.9324\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.1861 - accuracy: 0.9302 - val_loss: 0.2825 - val_accuracy: 0.9206\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1586 - accuracy: 0.9393 - val_loss: 0.2906 - val_accuracy: 0.9265\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1722 - accuracy: 0.9433 - val_loss: 0.3006 - val_accuracy: 0.9176\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1730 - accuracy: 0.9383 - val_loss: 0.2854 - val_accuracy: 0.9294\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1611 - accuracy: 0.9423 - val_loss: 0.2732 - val_accuracy: 0.9265\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1823 - accuracy: 0.9312 - val_loss: 0.2785 - val_accuracy: 0.9176\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 13s 423ms/step - loss: 0.1576 - accuracy: 0.9403 - val_loss: 0.3145 - val_accuracy: 0.9235\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 13s 423ms/step - loss: 0.1618 - accuracy: 0.9443 - val_loss: 0.2751 - val_accuracy: 0.9294\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1916 - accuracy: 0.9322 - val_loss: 0.2787 - val_accuracy: 0.9235\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 13s 423ms/step - loss: 0.1561 - accuracy: 0.9453 - val_loss: 0.2758 - val_accuracy: 0.9324\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 13s 423ms/step - loss: 0.1444 - accuracy: 0.9453 - val_loss: 0.2826 - val_accuracy: 0.9265\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 13s 426ms/step - loss: 0.1533 - accuracy: 0.9504 - val_loss: 0.2822 - val_accuracy: 0.9265\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 13s 425ms/step - loss: 0.1846 - accuracy: 0.9372 - val_loss: 0.2746 - val_accuracy: 0.9265\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1628 - accuracy: 0.9383 - val_loss: 0.3052 - val_accuracy: 0.9294\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 13s 424ms/step - loss: 0.1385 - accuracy: 0.9577 - val_loss: 0.2612 - val_accuracy: 0.9324\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1669 - accuracy: 0.9413 - val_loss: 0.2891 - val_accuracy: 0.9353\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 13s 425ms/step - loss: 0.1657 - accuracy: 0.9345 - val_loss: 0.2711 - val_accuracy: 0.9324\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 13s 421ms/step - loss: 0.1227 - accuracy: 0.9534 - val_loss: 0.3017 - val_accuracy: 0.9176\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 13s 425ms/step - loss: 0.1166 - accuracy: 0.9656 - val_loss: 0.2815 - val_accuracy: 0.9265\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.1247 - accuracy: 0.9494 - val_loss: 0.3332 - val_accuracy: 0.9206\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1470 - accuracy: 0.9524 - val_loss: 0.2795 - val_accuracy: 0.9235\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1602 - accuracy: 0.9443 - val_loss: 0.2990 - val_accuracy: 0.9235\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1465 - accuracy: 0.9524 - val_loss: 0.2909 - val_accuracy: 0.9235\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.1311 - accuracy: 0.9556 - val_loss: 0.2968 - val_accuracy: 0.9353\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.1420 - accuracy: 0.9494 - val_loss: 0.2815 - val_accuracy: 0.9294\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1721 - accuracy: 0.9413 - val_loss: 0.2567 - val_accuracy: 0.9294\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 13s 423ms/step - loss: 0.1379 - accuracy: 0.9534 - val_loss: 0.2857 - val_accuracy: 0.9265\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 13s 424ms/step - loss: 0.1449 - accuracy: 0.9524 - val_loss: 0.2826 - val_accuracy: 0.9235\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 13s 423ms/step - loss: 0.1285 - accuracy: 0.9575 - val_loss: 0.2798 - val_accuracy: 0.9294\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1466 - accuracy: 0.9555 - val_loss: 0.2765 - val_accuracy: 0.9382\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.1268 - accuracy: 0.9567 - val_loss: 0.3093 - val_accuracy: 0.9206\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1487 - accuracy: 0.9413 - val_loss: 0.2846 - val_accuracy: 0.9324\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.1134 - accuracy: 0.9666 - val_loss: 0.3053 - val_accuracy: 0.9147\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 13s 420ms/step - loss: 0.1507 - accuracy: 0.9453 - val_loss: 0.2880 - val_accuracy: 0.9206\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 13s 422ms/step - loss: 0.1260 - accuracy: 0.9545 - val_loss: 0.2821 - val_accuracy: 0.9235\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.1441 - accuracy: 0.9474 - val_loss: 0.2667 - val_accuracy: 0.9412\n",
            "[INFO] evaluating after fine-tuning...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       1.00      0.90      0.95        20\n",
            "   buttercup       1.00      1.00      1.00        16\n",
            "   coltsfoot       0.94      0.94      0.94        18\n",
            "     cowslip       0.86      0.86      0.86        21\n",
            "      crocus       0.83      1.00      0.91        15\n",
            "    daffodil       0.96      0.89      0.92        27\n",
            "       daisy       1.00      1.00      1.00        23\n",
            "   dandelion       1.00      0.96      0.98        23\n",
            "  fritillary       1.00      0.91      0.95        23\n",
            "        iris       1.00      1.00      1.00        20\n",
            "  lilyvalley       0.86      0.90      0.88        20\n",
            "       pansy       1.00      1.00      1.00        19\n",
            "    snowdrop       0.86      1.00      0.92        18\n",
            "   sunflower       1.00      1.00      1.00        19\n",
            "   tigerlily       0.94      0.94      0.94        16\n",
            "       tulip       0.79      0.86      0.83        22\n",
            "  windflower       1.00      0.90      0.95        20\n",
            "\n",
            "    accuracy                           0.94       340\n",
            "   macro avg       0.94      0.94      0.94       340\n",
            "weighted avg       0.95      0.94      0.94       340\n",
            "\n",
            "[INFO] serializing model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: flowers17.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sQqnorJ2Uk_"
      },
      "source": [
        "Additional accuracy can be obtained by performing more aggressive data augmentation and continually unfreezing more and more CONV blocks in VGG16. While fine-tuning is certainly more work than feature extraction, it also enables us to tune and modify the weights in our CNN to a particular dataset – something that feature extraction does not allow. Thus, when given enough training data, consider applying fine-tuning as you’ll likely obtain higher classification accuracy\n",
        "than simple feature extraction alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lnVRR8vojkd"
      },
      "source": [
        "# 3.0 Resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeZjEduJoloW"
      },
      "source": [
        "Kaiming He et al. won the ILSVRC 2015 challenge using a [Residual Network](https://arxiv.org/abs/1512.03385) (or ResNet), that delivered an astounding top-five error rate under 3.6%. The winning variant used an extremely deep CNN composed of 152 layers (other variants had 34, 50, and 101 layers). It confirmed the general trend: \n",
        "> models are getting deeper and deeper, with fewer and fewer parameters. \n",
        "\n",
        "The key to being able to train such a deep network is to use skip connections (also called **shortcut connections**): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack. Let’s see why this is useful.\n",
        "\n",
        "When training a neural network, the goal is to make it model a target function $h(x)$.\n",
        "If you add the input $x$ to the output of the network (i.e., you add a skip connection), then the network will be forced to model $f(x) = h(x) – x$ rather than $h(x)$. This is called **residual learning**.\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1GwKaS8TEt6q5jL_z_MyvThESoNfpU__g\"></center><center>Residual learning.</center>\n",
        "\n",
        "\n",
        "When you initialize a regular neural network, its weights are close to zero, so the network just outputs values close to zero. If you add a skip connection, the resulting network just outputs a copy of its inputs; in other words, it initially models the identity function. If the target function is fairly close to the identity function (which is often the case), this will speed up training considerably.\n",
        "\n",
        "\n",
        "Moreover, if you add many skip connections, the network can start making progress\n",
        "even if several layers have not started learning yet. Thanks to skip connections, the signal can easily make its way across the whole network. The deep residual network can be seen as a stack of **residual units (RUs)**, where each residual\n",
        "unit is a small neural network with a skip connection.\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1MyfGAHM7ncWz1s-xwRf7ngwYmzffot78\"></center><center>Regular deep neural network (left) and deep residual network (right).</center>\n",
        "\n",
        "\n",
        "The Resnet architecture is surprisingly simple. Each residual unit is composed\n",
        "of two convolutional layers (and no pooling layer!), with Batch Normalization\n",
        "(BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions\n",
        "(stride 1, \"same\" padding).\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=11iT7HyK_qiMRr8zZuBZvNJXl3sEv5brV\"></center><center>Resnet architecture.</center>\n",
        "\n",
        "\n",
        "Note that the number of feature maps is doubled every few residual units, at the same\n",
        "time as their height and width are halved (using a convolutional layer with stride 2).\n",
        "When this happens, the inputs cannot be added directly to the outputs of the residual\n",
        "unit because they don’t have the same shape (for example, this problem affects the\n",
        "skip connection represented by the dashed arrow in Figure \"Resnet architecture\"). To solve this problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right number of output feature maps (see Figure below).\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1eKg1nuGhJNwtKFMmmX_Nxve29p5wdPTU\"></center><center>Skip connection when changing feature map size and depth.</center>\n",
        "\n",
        "A complete reference for Resnet architectures can be visualized in Figure below (original paper).\n",
        "\n",
        "<center><img width=\"700\" src=\"https://drive.google.com/uc?export=view&id=10CJACf-yLsi3rWai-a831a3FdqZXP8zl\"></center><center>Different Resnet configurations (original paper).</center>\n"
      ]
    }
  ]
}